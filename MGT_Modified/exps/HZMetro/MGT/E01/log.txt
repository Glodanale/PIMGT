Start time: 2024-09-22 14:53:36.816988
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 2,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
Start time: 2024-09-25 11:45:40.350663
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 2,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 594
[epoch 0/99] ave_loss: 63.297209, time_elapsed: 165.376460(sec)
Validating...
num_samples: 132, num_batches: 66
time_elapsed: 4.434536(sec)
The best model 'best.pth' has been updated
mae: 37.149185, best_mae: 37.149185
Start time: 2024-09-25 12:52:16.025203
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 2,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 594
[epoch 0/99] ave_loss: 64.596771, time_elapsed: 149.186712(sec)
Validating...
num_samples: 132, num_batches: 66
time_elapsed: 5.113448(sec)
The best model 'best.pth' has been updated
mae: 35.635639, best_mae: 35.635639
[epoch 1/99] ave_loss: 46.004839, time_elapsed: 149.013897(sec)
Validating...
num_samples: 132, num_batches: 66
time_elapsed: 5.106397(sec)
mae: 38.434364, best_mae: 35.635639
[epoch 2/99] ave_loss: 40.405626, time_elapsed: 172.883978(sec)
Validating...
num_samples: 132, num_batches: 66
time_elapsed: 8.300903(sec)
mae: 37.419052, best_mae: 35.635639
[epoch 3/99] ave_loss: 37.293534, time_elapsed: 259.721433(sec)
Validating...
num_samples: 132, num_batches: 66
time_elapsed: 8.549220(sec)
The best model 'best.pth' has been updated
mae: 29.646111, best_mae: 29.646111
[epoch 4/99] ave_loss: 35.301094, time_elapsed: 302.405048(sec)
Validating...
num_samples: 132, num_batches: 66
time_elapsed: 8.892445(sec)
mae: 31.815437, best_mae: 29.646111
[epoch 5/99] ave_loss: 34.235114, time_elapsed: 319.107421(sec)
Validating...
num_samples: 132, num_batches: 66
time_elapsed: 9.404114(sec)
mae: 35.588646, best_mae: 29.646111
[epoch 6/99] ave_loss: 32.838331, time_elapsed: 334.021508(sec)
Validating...
num_samples: 132, num_batches: 66
time_elapsed: 10.104594(sec)
mae: 33.824757, best_mae: 29.646111
[epoch 7/99] ave_loss: 32.724778, time_elapsed: 367.319117(sec)
Validating...
num_samples: 132, num_batches: 66
time_elapsed: 10.520793(sec)
mae: 32.696808, best_mae: 29.646111
[epoch 8/99] ave_loss: 29.988816, time_elapsed: 380.708458(sec)
Validating...
num_samples: 132, num_batches: 66
time_elapsed: 10.793276(sec)
The best model 'best.pth' has been updated
mae: 28.388756, best_mae: 28.388756
Start time: 2024-09-25 16:27:10.262814
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 2,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 594
[epoch 0/99] ave_loss: 66.536726, time_elapsed: 155.844237(sec)
Validating...
num_samples: 132, num_batches: 66
time_elapsed: 5.370479(sec)
The best model 'best.pth' has been updated
mae: 36.845619, best_mae: 36.845619
Start time: 2024-09-25 16:30:08.653919
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 2,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 594
Start time: 2024-09-25 16:48:06.094689
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 2,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 594
[epoch 0/99] ave_loss: 69.386062, time_elapsed: 143.449845(sec)
Validating...
num_samples: 132, num_batches: 66
time_elapsed: 5.139148(sec)
The best model 'best.pth' has been updated
mae: 35.674919, best_mae: 35.674919
Start time: 2024-09-25 16:53:04.071809
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 2,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 594
[epoch 0/99] ave_loss: 66.321949, time_elapsed: 141.736284(sec)
Validating...
num_samples: 132, num_batches: 66
time_elapsed: 4.984507(sec)
The best model 'best.pth' has been updated
mae: 36.621353, best_mae: 36.621353
[epoch 1/99] ave_loss: 44.369343, time_elapsed: 143.895500(sec)
Validating...
num_samples: 132, num_batches: 66
time_elapsed: 4.972648(sec)
The best model 'best.pth' has been updated
mae: 34.934921, best_mae: 34.934921
[epoch 2/99] ave_loss: 40.889303, time_elapsed: 161.728412(sec)
Validating...
num_samples: 132, num_batches: 66
time_elapsed: 7.596709(sec)
The best model 'best.pth' has been updated
mae: 34.814629, best_mae: 34.814629
[epoch 3/99] ave_loss: 39.125472, time_elapsed: 266.901492(sec)
Validating...
num_samples: 132, num_batches: 66
time_elapsed: 10.172277(sec)
The best model 'best.pth' has been updated
mae: 30.170879, best_mae: 30.170879
[epoch 4/99] ave_loss: 35.435431, time_elapsed: 363.002624(sec)
Validating...
num_samples: 132, num_batches: 66
time_elapsed: 10.604939(sec)
The best model 'best.pth' has been updated
mae: 29.505779, best_mae: 29.505779
Start time: 2024-09-27 13:48:07.331562
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 2,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 594
Start time: 2024-09-27 13:52:01.351599
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 2,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 594
[epoch 0/99] ave_loss: 65.775644, time_elapsed: 161.690743(sec)
Validating...
num_samples: 132, num_batches: 66
time_elapsed: 5.481482(sec)
The best model 'best.pth' has been updated
mae: 48.571960, best_mae: 48.571960
[epoch 1/99] ave_loss: 46.020887, time_elapsed: 158.497638(sec)
Validating...
num_samples: 132, num_batches: 66
time_elapsed: 5.374587(sec)
The best model 'best.pth' has been updated
mae: 36.148163, best_mae: 36.148163
[epoch 2/99] ave_loss: 40.672874, time_elapsed: 180.392825(sec)
Validating...
num_samples: 132, num_batches: 66
time_elapsed: 8.157224(sec)
The best model 'best.pth' has been updated
mae: 36.127892, best_mae: 36.127892
[epoch 3/99] ave_loss: 38.278751, time_elapsed: 282.373791(sec)
Validating...
num_samples: 132, num_batches: 66
time_elapsed: 10.054439(sec)
The best model 'best.pth' has been updated
mae: 35.633198, best_mae: 35.633198
[epoch 4/99] ave_loss: 36.446383, time_elapsed: 356.779146(sec)
Validating...
num_samples: 132, num_batches: 66
time_elapsed: 10.574466(sec)
The best model 'best.pth' has been updated
mae: 31.929230, best_mae: 31.929230
[epoch 5/99] ave_loss: 34.510171, time_elapsed: 393.634363(sec)
Validating...
num_samples: 132, num_batches: 66
Start time: 2024-09-27 14:18:44.676817
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 2,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 594
Start time: 2024-09-27 14:26:29.445931
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 2,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 594
Start time: 2024-09-27 14:56:30.603587
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 2,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 594
Start time: 2024-09-27 14:57:34.046269
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 2,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 594
Start time: 2024-09-27 14:58:14.725505
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 2,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 594
Start time: 2024-09-27 15:10:43.389338
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 2,
  "num_workers": 32,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 594
Start time: 2024-09-27 15:11:29.138223
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 2,
  "num_workers": 32,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 594
Start time: 2024-09-27 15:12:31.746110
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
Start time: 2024-09-27 15:13:39.191889
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 32,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 132.531417, time_elapsed: 45.329080(sec)
Validating...
num_samples: 132, num_batches: 5
time_elapsed: 2.343945(sec)
The best model 'best.pth' has been updated
mae: 56.730453, best_mae: 56.730453
Start time: 2024-09-27 15:14:56.969568
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 32,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 145.982119, time_elapsed: 45.481501(sec)
Validating...
num_samples: 132, num_batches: 5
time_elapsed: 2.449822(sec)
The best model 'best.pth' has been updated
mae: 67.033676, best_mae: 67.033676
Start time: 2024-09-27 15:18:33.414336
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 32,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 106.865154, time_elapsed: 44.882074(sec)
Validating...
num_samples: 132, num_batches: 5
time_elapsed: 2.368520(sec)
The best model 'best.pth' has been updated
mae: 50.784969, best_mae: 50.784969
Start time: 2024-09-27 15:21:23.462413
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 16,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 124.811917, time_elapsed: 44.421980(sec)
Validating...
num_samples: 132, num_batches: 5
time_elapsed: 2.098032(sec)
The best model 'best.pth' has been updated
mae: 59.869675, best_mae: 59.869675
Start time: 2024-09-27 15:24:32.390439
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 16,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 87.497019, time_elapsed: 50.488694(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 2.183333(sec)
The best model 'best.pth' has been updated
mae: 47.704094, best_mae: 47.704094
[epoch 1/99] ave_loss: 53.269579, time_elapsed: 53.828575(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 2.353793(sec)
The best model 'best.pth' has been updated
mae: 36.026894, best_mae: 36.026894
[epoch 2/99] ave_loss: 44.929916, time_elapsed: 53.867118(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 2.363856(sec)
The best model 'best.pth' has been updated
mae: 32.652748, best_mae: 32.652748
[epoch 3/99] ave_loss: 40.973219, time_elapsed: 54.767565(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 2.347118(sec)
The best model 'best.pth' has been updated
mae: 30.845703, best_mae: 30.845703
[epoch 4/99] ave_loss: 39.357587, time_elapsed: 54.207007(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 2.276737(sec)
The best model 'best.pth' has been updated
mae: 30.275816, best_mae: 30.275816
[epoch 5/99] ave_loss: 37.658291, time_elapsed: 54.197141(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 2.407053(sec)
The best model 'best.pth' has been updated
mae: 29.467503, best_mae: 29.467503
[epoch 6/99] ave_loss: 35.806450, time_elapsed: 54.582333(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 2.298695(sec)
The best model 'best.pth' has been updated
mae: 28.765779, best_mae: 28.765779
[epoch 7/99] ave_loss: 35.817557, time_elapsed: 54.828665(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 2.362870(sec)
The best model 'best.pth' has been updated
mae: 28.763613, best_mae: 28.763613
[epoch 8/99] ave_loss: 33.855599, time_elapsed: 54.089622(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 2.269506(sec)
The best model 'best.pth' has been updated
mae: 28.361681, best_mae: 28.361681
[epoch 9/99] ave_loss: 33.988277, time_elapsed: 53.804003(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 2.312936(sec)
mae: 28.488543, best_mae: 28.361681
[epoch 10/99] ave_loss: 33.459293, time_elapsed: 53.928265(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 2.408060(sec)
mae: 29.657793, best_mae: 28.361681
[epoch 11/99] ave_loss: 32.079043, time_elapsed: 53.816783(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 2.457978(sec)
mae: 28.590569, best_mae: 28.361681
[epoch 12/99] ave_loss: 31.209091, time_elapsed: 54.399346(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 2.346560(sec)
The best model 'best.pth' has been updated
mae: 27.997599, best_mae: 27.997599
[epoch 13/99] ave_loss: 30.829082, time_elapsed: 53.645169(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 2.366378(sec)
The best model 'best.pth' has been updated
mae: 27.331673, best_mae: 27.331673
[epoch 14/99] ave_loss: 31.891190, time_elapsed: 55.033211(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 2.425777(sec)
The best model 'best.pth' has been updated
mae: 26.934134, best_mae: 26.934134
[epoch 15/99] ave_loss: 30.243553, time_elapsed: 55.474985(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 2.351358(sec)
mae: 27.389830, best_mae: 26.934134
[epoch 16/99] ave_loss: 29.877850, time_elapsed: 54.553293(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 2.326312(sec)
The best model 'best.pth' has been updated
mae: 26.730217, best_mae: 26.730217
[epoch 17/99] ave_loss: 28.791568, time_elapsed: 55.201904(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 2.386694(sec)
mae: 26.949915, best_mae: 26.730217
[epoch 18/99] ave_loss: 30.616605, time_elapsed: 54.710458(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 2.342520(sec)
mae: 27.825169, best_mae: 26.730217
[epoch 19/99] ave_loss: 28.380931, time_elapsed: 55.446526(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 2.344118(sec)
The best model 'best.pth' has been updated
mae: 25.485710, best_mae: 25.485710
[epoch 20/99] ave_loss: 28.116845, time_elapsed: 54.904371(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 2.500046(sec)
mae: 25.730562, best_mae: 25.485710
[epoch 21/99] ave_loss: 28.283957, time_elapsed: 64.259608(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 3.247332(sec)
mae: 27.630966, best_mae: 25.485710
[epoch 22/99] ave_loss: 28.216832, time_elapsed: 80.337941(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 3.645400(sec)
mae: 27.031044, best_mae: 25.485710
[epoch 23/99] ave_loss: 27.642760, time_elapsed: 92.994513(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 4.039989(sec)
mae: 27.009888, best_mae: 25.485710
[epoch 24/99] ave_loss: 27.793882, time_elapsed: 102.342791(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 4.322029(sec)
mae: 26.466238, best_mae: 25.485710
[epoch 25/99] ave_loss: 26.897465, time_elapsed: 112.553172(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 4.435708(sec)
mae: 26.429775, best_mae: 25.485710
[epoch 26/99] ave_loss: 26.351039, time_elapsed: 121.351367(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 4.559320(sec)
mae: 26.133911, best_mae: 25.485710
[epoch 27/99] ave_loss: 26.816795, time_elapsed: 128.605614(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 4.772987(sec)
mae: 25.704889, best_mae: 25.485710
[epoch 28/99] ave_loss: 27.118977, time_elapsed: 136.825714(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 4.592962(sec)
mae: 26.839798, best_mae: 25.485710
Start time: 2024-09-27 16:27:20.398564
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 16,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
Start time: 2024-09-27 16:38:15.835130
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 16,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
Start time: 2024-09-27 16:40:08.537672
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 16,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
Start time: 2024-09-27 18:35:32.143784
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 16,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 94.985978, time_elapsed: 15.602845(sec)
Validating...
num_samples: 132, num_batches: 9
Start time: 2024-09-27 18:37:40.130219
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 16,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 117.668262, time_elapsed: 15.410941(sec)
Validating...
num_samples: 132, num_batches: 9
Start time: 2024-09-27 18:39:32.623914
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 2,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 594
[epoch 0/99] ave_loss: 66.305422, time_elapsed: 101.883532(sec)
Validating...
num_samples: 132, num_batches: 66
Start time: 2024-09-27 18:42:55.024917
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 16,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 93.794276, time_elapsed: 15.198948(sec)
Validating...
num_samples: 132, num_batches: 9
Start time: 2024-09-27 18:49:14.670999
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 16,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 92.354481, time_elapsed: 15.706506(sec)
Validating...
num_samples: 132, num_batches: 9
Start time: 2024-09-27 18:57:42.617517
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 16,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 92.251111, time_elapsed: 16.221540(sec)
Validating...
num_samples: 132, num_batches: 9
Start time: 2024-09-27 19:08:28.466237
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 16,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 112.115716, time_elapsed: 15.577088(sec)
Validating...
num_samples: 132, num_batches: 9
Start time: 2024-09-27 19:19:06.437988
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 16,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
Start time: 2024-09-27 19:20:11.588048
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 16,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 93.362913, time_elapsed: 15.372873(sec)
Validating...
num_samples: 132, num_batches: 9
Start time: 2024-09-27 22:53:35.910089
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 16,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 93.732260, time_elapsed: 15.400505(sec)
Validating...
num_samples: 132, num_batches: 9
Start time: 2024-09-27 23:00:33.523510
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 2,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 594
[epoch 0/99] ave_loss: 68.977521, time_elapsed: 99.033692(sec)
Validating...
num_samples: 132, num_batches: 66
Start time: 2024-09-27 23:03:38.211119
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 2,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 594
[epoch 0/99] ave_loss: 63.643659, time_elapsed: 103.681019(sec)
Validating...
num_samples: 132, num_batches: 66
Start time: 2024-09-27 23:07:09.166697
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 16,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 91.652066, time_elapsed: 15.990612(sec)
Validating...
num_samples: 132, num_batches: 9
Start time: 2024-10-03 23:36:24.214723
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 16,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 104.343658, time_elapsed: 15.018799(sec)
Validating...
num_samples: 132, num_batches: 9
Fatal Python error: Segmentation fault

Thread 0x00007ff3883e4640 (most recent call first):
  <no Python frame>

Thread 0x00007ff2f9bff640 (most recent call first):
  File "/home/glodanale/miniconda3/envs/MGT/lib/python3.12/threading.py", line 359 in wait
  File "/home/glodanale/miniconda3/envs/MGT/lib/python3.12/threading.py", line 655 in wait
  File "/home/glodanale/miniconda3/envs/MGT/lib/python3.12/site-packages/tqdm/_monitor.py", line 60 in run
  File "/home/glodanale/miniconda3/envs/MGT/lib/python3.12/threading.py", line 1075 in _bootstrap_inner
  File "/home/glodanale/miniconda3/envs/MGT/lib/python3.12/threading.py", line 1032 in _bootstrap

Current thread 0x00007ff468091740 (most recent call first):
  File "/home/glodanale/MGT/utils/Metrics.py", line 14 in masked_mse_np
  File "/home/glodanale/MGT/utils/Metrics.py", line 6 in masked_rmse_np
  File "/home/glodanale/MGT/utils/Metrics.py", line 58 in rmse
  File "/home/glodanale/MGT/utils/Metrics.py", line 87 in all
  File "/home/glodanale/MGT/main.py", line 129 in val
  File "/home/glodanale/MGT/main.py", line 179 in train
  File "/home/glodanale/MGT/main.py", line 254 in <module>

Extension modules: yaml._yaml, mkl._mklinit, mkl._py_mkl_service, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, numpy._core._multiarray_umath, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pandas._libs.ops, numexpr.interpreter, bottleneck.move, bottleneck.nonreduce, bottleneck.nonreduce_axis, bottleneck.reduce, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering (total: 96)
Start time: 2024-10-03 23:40:54.556186
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 16,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 95.326416, time_elapsed: 15.574219(sec)
Validating...
num_samples: 132, num_batches: 9
Start time: 2024-10-03 23:45:37.587808
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 16,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
Start time: 2024-10-03 23:45:59.681629
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 16,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
Start time: 2024-10-03 23:48:25.556216
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 16,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 95.818685, time_elapsed: 14.757552(sec)
Validating...
num_samples: 132, num_batches: 9
Fatal Python error: Segmentation fault

Thread 0x00007ff9747e4640 (most recent call first):
  <no Python frame>

Thread 0x00007ff8fce6e640 (most recent call first):
  File "/home/glodanale/miniconda3/envs/MGT/lib/python3.12/threading.py", line 359 in wait
  File "/home/glodanale/miniconda3/envs/MGT/lib/python3.12/threading.py", line 655 in wait
  File "/home/glodanale/miniconda3/envs/MGT/lib/python3.12/site-packages/tqdm/_monitor.py", line 60 in run
  File "/home/glodanale/miniconda3/envs/MGT/lib/python3.12/threading.py", line 1075 in _bootstrap_inner
  File "/home/glodanale/miniconda3/envs/MGT/lib/python3.12/threading.py", line 1032 in _bootstrap

Current thread 0x00007ffa7633b740 (most recent call first):
  File "/home/glodanale/MGT/utils/Metrics.py", line 14 in masked_mse_np
  File "/home/glodanale/MGT/utils/Metrics.py", line 6 in masked_rmse_np
  File "/home/glodanale/MGT/utils/Metrics.py", line 58 in rmse
  File "/home/glodanale/MGT/utils/Metrics.py", line 87 in all
  File "/home/glodanale/MGT/main.py", line 129 in val
  File "/home/glodanale/MGT/main.py", line 181 in train
  File "/home/glodanale/MGT/main.py", line 264 in <module>

Extension modules: yaml._yaml, mkl._mklinit, mkl._py_mkl_service, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, numpy._core._multiarray_umath, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pandas._libs.ops, numexpr.interpreter, bottleneck.move, bottleneck.nonreduce, bottleneck.nonreduce_axis, bottleneck.reduce, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering (total: 96)
Start time: 2024-10-03 23:49:50.026624
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 16,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 109.367212, time_elapsed: 15.088254(sec)
Validating...
num_samples: 132, num_batches: 9
Fatal Python error: Segmentation fault

Thread 0x00007f7a64d10640 (most recent call first):
  <no Python frame>

Thread 0x00007f79cd96e640 (most recent call first):
  File "/home/glodanale/miniconda3/envs/MGT/lib/python3.12/threading.py", line 359 in wait
  File "/home/glodanale/miniconda3/envs/MGT/lib/python3.12/threading.py", line 655 in wait
  File "/home/glodanale/miniconda3/envs/MGT/lib/python3.12/site-packages/tqdm/_monitor.py", line 60 in run
  File "/home/glodanale/miniconda3/envs/MGT/lib/python3.12/threading.py", line 1075 in _bootstrap_inner
  File "/home/glodanale/miniconda3/envs/MGT/lib/python3.12/threading.py", line 1032 in _bootstrap

Current thread 0x00007f7b46eab740 (most recent call first):
  File "/home/glodanale/MGT/utils/Metrics.py", line 14 in masked_mse_np
  File "/home/glodanale/MGT/utils/Metrics.py", line 6 in masked_rmse_np
  File "/home/glodanale/MGT/utils/Metrics.py", line 58 in rmse
  File "/home/glodanale/MGT/utils/Metrics.py", line 87 in all
  File "/home/glodanale/MGT/main.py", line 129 in val
  File "/home/glodanale/MGT/main.py", line 181 in train
  File "/home/glodanale/MGT/main.py", line 264 in <module>

Extension modules: yaml._yaml, mkl._mklinit, mkl._py_mkl_service, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, numpy._core._multiarray_umath, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pandas._libs.ops, numexpr.interpreter, bottleneck.move, bottleneck.nonreduce, bottleneck.nonreduce_axis, bottleneck.reduce, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering (total: 96)
Start time: 2024-10-03 23:52:14.438112
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 16,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 104.169023, time_elapsed: 15.727397(sec)
Validating...
num_samples: 132, num_batches: 9
Fatal Python error: Segmentation fault

Thread 0x00007fbbdd943640 (most recent call first):
  <no Python frame>

Thread 0x00007fbb4be6e640 (most recent call first):
  File "/home/glodanale/miniconda3/envs/MGT/lib/python3.12/threading.py", line 359 in wait
  File "/home/glodanale/miniconda3/envs/MGT/lib/python3.12/threading.py", line 655 in wait
  File "/home/glodanale/miniconda3/envs/MGT/lib/python3.12/site-packages/tqdm/_monitor.py", line 60 in run
  File "/home/glodanale/miniconda3/envs/MGT/lib/python3.12/threading.py", line 1075 in _bootstrap_inner
  File "/home/glodanale/miniconda3/envs/MGT/lib/python3.12/threading.py", line 1032 in _bootstrap

Current thread 0x00007fbcc5469740 (most recent call first):
  File "/home/glodanale/MGT/utils/Metrics.py", line 14 in masked_mse_np
  File "/home/glodanale/MGT/utils/Metrics.py", line 6 in masked_rmse_np
  File "/home/glodanale/MGT/utils/Metrics.py", line 58 in rmse
  File "/home/glodanale/MGT/utils/Metrics.py", line 87 in all
  File "/home/glodanale/MGT/main.py", line 129 in val
  File "/home/glodanale/MGT/main.py", line 181 in train
  File "/home/glodanale/MGT/main.py", line 261 in <module>

Extension modules: yaml._yaml, mkl._mklinit, mkl._py_mkl_service, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, numpy._core._multiarray_umath, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pandas._libs.ops, numexpr.interpreter, bottleneck.move, bottleneck.nonreduce, bottleneck.nonreduce_axis, bottleneck.reduce, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering (total: 96)
Start time: 2024-10-03 23:53:19.727644
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 16,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 111.512045, time_elapsed: 15.209214(sec)
Validating...
num_samples: 132, num_batches: 9
Fatal Python error: Segmentation fault

Thread 0x00007fea6c143640 (most recent call first):
  <no Python frame>

Thread 0x00007fe9e876e640 (most recent call first):
  File "/home/glodanale/miniconda3/envs/MGT/lib/python3.12/threading.py", line 359 in wait
  File "/home/glodanale/miniconda3/envs/MGT/lib/python3.12/threading.py", line 655 in wait
  File "/home/glodanale/miniconda3/envs/MGT/lib/python3.12/site-packages/tqdm/_monitor.py", line 60 in run
  File "/home/glodanale/miniconda3/envs/MGT/lib/python3.12/threading.py", line 1075 in _bootstrap_inner
  File "/home/glodanale/miniconda3/envs/MGT/lib/python3.12/threading.py", line 1032 in _bootstrap

Current thread 0x00007feb61b6c740 (most recent call first):
  File "/home/glodanale/MGT/utils/Metrics.py", line 14 in masked_mse_np
  File "/home/glodanale/MGT/utils/Metrics.py", line 6 in masked_rmse_np
  File "/home/glodanale/MGT/utils/Metrics.py", line 58 in rmse
  File "/home/glodanale/MGT/utils/Metrics.py", line 87 in all
  File "/home/glodanale/MGT/main.py", line 129 in val
  File "/home/glodanale/MGT/main.py", line 181 in train
  File "/home/glodanale/MGT/main.py", line 261 in <module>

Extension modules: yaml._yaml, mkl._mklinit, mkl._py_mkl_service, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, numpy._core._multiarray_umath, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pandas._libs.ops, numexpr.interpreter, bottleneck.move, bottleneck.nonreduce, bottleneck.nonreduce_axis, bottleneck.reduce, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering (total: 96)
Start time: 2024-10-03 23:56:41.174017
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 16,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 103.769732, time_elapsed: 15.332511(sec)
Validating...
num_samples: 132, num_batches: 9
Start time: 2024-10-04 00:02:07.117078
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 124.944269, time_elapsed: 13.609697(sec)
Validating...
num_samples: 132, num_batches: 9
Start time: 2024-10-04 13:53:12.874771
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
Start time: 2024-10-04 13:53:43.347390
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 2,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 105.283353, time_elapsed: 12.631989(sec)
Validating...
num_samples: 132, num_batches: 9
Start time: 2024-10-04 13:54:45.500849
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 1,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 102.079783, time_elapsed: 12.741329(sec)
Validating...
num_samples: 132, num_batches: 9
Start time: 2024-10-04 14:03:38.098851
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 1,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 100.450622, time_elapsed: 12.475765(sec)
Validating...
num_samples: 132, num_batches: 9
---------- Testing ----------
num_samples: 330, num_batches: 21
Start time: 2024-10-04 14:07:53.977375
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 1,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 96.203083, time_elapsed: 12.703267(sec)
Validating...
num_samples: 132, num_batches: 9
Start time: 2024-10-04 14:13:32.272440
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 1,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 96.122582, time_elapsed: 12.502815(sec)
Validating...
num_samples: 132, num_batches: 9
---------- Testing ----------
num_samples: 330, num_batches: 21
Start time: 2024-10-04 14:14:04.646869
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 1,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 111.204568, time_elapsed: 12.373717(sec)
Validating...
num_samples: 132, num_batches: 9
Start time: 2024-10-04 14:19:55.166331
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 1,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 107.500001, time_elapsed: 12.697245(sec)
Validating...
num_samples: 132, num_batches: 9
Start time: 2024-10-04 14:26:14.679051
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 1,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 92.747327, time_elapsed: 12.648673(sec)
Validating...
num_samples: 132, num_batches: 9
---------- Testing ----------
num_samples: 330, num_batches: 21
Start time: 2024-10-04 14:33:47.115425
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 1,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
Start time: 2024-10-04 14:35:49.087374
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 1,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
Start time: 2024-10-04 14:37:45.194265
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 1,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
Start time: 2024-10-04 14:41:55.043421
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 1,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 101.945753, time_elapsed: 12.736400(sec)
Validating...
num_samples: 132, num_batches: 9
Start time: 2024-10-04 14:43:47.901553
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 1,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 90.457949, time_elapsed: 12.674028(sec)
Validating...
num_samples: 132, num_batches: 9
Start time: 2024-10-04 14:50:23.205291
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 1,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 96.045211, time_elapsed: 13.366119(sec)
Validating...
num_samples: 132, num_batches: 9
Start time: 2024-10-04 15:00:42.985204
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 1,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 109.168845, time_elapsed: 12.916847(sec)
Validating...
num_samples: 132, num_batches: 9
Start time: 2024-10-04 15:05:34.559401
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 2,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 594
[epoch 0/99] ave_loss: 65.345201, time_elapsed: 101.462896(sec)
Validating...
num_samples: 132, num_batches: 66
Start time: 2024-10-04 15:19:58.837101
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 121.332270, time_elapsed: 9.303963(sec)
Validating...
num_samples: 132, num_batches: 5
Start time: 2024-10-04 15:21:43.862399
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 132.107172, time_elapsed: 9.195452(sec)
Validating...
num_samples: 132, num_batches: 5
Start time: 2024-10-04 15:27:07.045927
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 145.212783, time_elapsed: 10.081985(sec)
Validating...
num_samples: 132, num_batches: 5
Start time: 2024-10-04 15:27:35.918984
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 122.279309, time_elapsed: 9.008856(sec)
Validating...
num_samples: 132, num_batches: 5
Start time: 2024-10-04 15:28:37.295680
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 122.578154, time_elapsed: 8.940340(sec)
Validating...
num_samples: 132, num_batches: 5
Start time: 2024-10-04 15:29:13.312807
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 120.204000, time_elapsed: 9.259632(sec)
Validating...
num_samples: 132, num_batches: 5
Start time: 2024-10-04 15:30:19.202997
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 127.586319, time_elapsed: 9.203044(sec)
Validating...
num_samples: 132, num_batches: 5
Start time: 2024-10-04 15:41:26.351294
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 118.103997, time_elapsed: 9.306450(sec)
Validating...
num_samples: 132, num_batches: 5
Start time: 2024-10-04 16:28:06.992286
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 104.030779, time_elapsed: 9.618237(sec)
Validating...
num_samples: 132, num_batches: 5
Start time: 2024-10-04 16:28:44.186848
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 113.232117, time_elapsed: 9.151025(sec)
Validating...
num_samples: 132, num_batches: 5
Start time: 2024-10-04 16:30:54.229922
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 104.241742, time_elapsed: 8.969909(sec)
Validating...
num_samples: 132, num_batches: 5
Start time: 2024-10-04 16:42:53.003267
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 101.265715, time_elapsed: 9.045852(sec)
Validating...
num_samples: 132, num_batches: 5
Start time: 2024-10-04 16:49:26.587106
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 127.694063, time_elapsed: 9.058353(sec)
Validating...
num_samples: 132, num_batches: 5
Start time: 2024-10-04 16:50:54.472668
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 132.528801, time_elapsed: 9.687348(sec)
Validating...
num_samples: 132, num_batches: 5
Start time: 2024-10-04 16:56:07.294294
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 130.602135, time_elapsed: 8.898150(sec)
Validating...
num_samples: 132, num_batches: 5
Start time: 2024-10-04 16:56:50.069334
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 120.255054, time_elapsed: 9.822888(sec)
Validating...
num_samples: 132, num_batches: 5
Start time: 2024-10-24 16:46:03.037983
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 152.787420, time_elapsed: 8.562800(sec)
Validating...
num_samples: 132, num_batches: 5
Start time: 2024-10-24 17:46:47.325448
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 117.727502, time_elapsed: 8.638992(sec)
Validating...
num_samples: 132, num_batches: 5
Start time: 2024-10-24 18:10:56.804137
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 134.383469, time_elapsed: 8.547594(sec)
Validating...
num_samples: 132, num_batches: 5
Start time: 2024-10-24 18:43:26.997359
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 126.666265, time_elapsed: 9.093694(sec)
Validating...
num_samples: 132, num_batches: 5
Start time: 2024-10-25 13:01:06.298998
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 124.239114, time_elapsed: 8.749977(sec)
Validating...
num_samples: 132, num_batches: 5
Start time: 2025-02-03 22:28:21.707791
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 141.185378, time_elapsed: 44.509437(sec)
Validating...
num_samples: 132, num_batches: 5
time_elapsed: 1.930933(sec)
The best model 'best.pth' has been updated
mae: 52.708939, best_mae: 52.708939
Start time: 2025-02-17 20:58:24.005301
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 138.469066, time_elapsed: 47.887441(sec)
Validating...
num_samples: 132, num_batches: 5
time_elapsed: 1.899856(sec)
The best model 'best.pth' has been updated
mae: 50.695602, best_mae: 50.695602
Start time: 2025-02-17 21:52:18.850190
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 111.537336, time_elapsed: 47.437610(sec)
Validating...
num_samples: 132, num_batches: 5
time_elapsed: 1.853504(sec)
The best model 'best.pth' has been updated
mae: 52.487705, best_mae: 52.487705
Start time: 2025-02-17 21:56:40.834960
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 122.872901, time_elapsed: 49.970941(sec)
Validating...
num_samples: 132, num_batches: 5
time_elapsed: 2.287228(sec)
The best model 'best.pth' has been updated
mae: 53.597755, best_mae: 53.597755
Start time: 2025-02-17 22:00:28.605048
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 8,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 148
[epoch 0/99] ave_loss: 72.911230, time_elapsed: 69.933043(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: 2.411678(sec)
The best model 'best.pth' has been updated
mae: 43.214329, best_mae: 43.214329
[epoch 1/99] ave_loss: 47.566332, time_elapsed: 73.013901(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: 2.433779(sec)
The best model 'best.pth' has been updated
mae: 34.541561, best_mae: 34.541561
[epoch 2/99] ave_loss: 41.710722, time_elapsed: 72.853078(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: 3.063143(sec)
The best model 'best.pth' has been updated
mae: 31.050903, best_mae: 31.050903
[epoch 3/99] ave_loss: 38.317289, time_elapsed: 73.081435(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: 2.970213(sec)
The best model 'best.pth' has been updated
mae: 30.262791, best_mae: 30.262791
[epoch 4/99] ave_loss: 36.956469, time_elapsed: 80.512709(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: -4.018529(sec)
mae: 31.577019, best_mae: 30.262791
[epoch 5/99] ave_loss: 35.591611, time_elapsed: 72.833178(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: 2.426331(sec)
mae: 35.355343, best_mae: 30.262791
[epoch 6/99] ave_loss: 33.856875, time_elapsed: 73.712858(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: 2.526082(sec)
The best model 'best.pth' has been updated
mae: 29.425938, best_mae: 29.425938
Start time: 2025-02-17 22:10:47.881607
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 8,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 148
[epoch 0/99] ave_loss: 79.487957, time_elapsed: 69.974370(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: 2.344187(sec)
The best model 'best.pth' has been updated
mae: 41.574127, best_mae: 41.574127
[epoch 1/99] ave_loss: 48.707733, time_elapsed: 73.912391(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: 2.412564(sec)
The best model 'best.pth' has been updated
mae: 34.025394, best_mae: 34.025394
[epoch 2/99] ave_loss: 42.551034, time_elapsed: 79.036559(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: -2.596051(sec)
mae: 34.749882, best_mae: 34.025394
[epoch 3/99] ave_loss: 39.903016, time_elapsed: 79.802205(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: 2.985647(sec)
The best model 'best.pth' has been updated
mae: 31.163589, best_mae: 31.163589
[epoch 4/99] ave_loss: 37.454353, time_elapsed: 66.541457(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: 3.012120(sec)
The best model 'best.pth' has been updated
mae: 30.259354, best_mae: 30.259354
[epoch 5/99] ave_loss: 35.639514, time_elapsed: 72.529473(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: 8.111060(sec)
The best model 'best.pth' has been updated
mae: 30.248156, best_mae: 30.248156
[epoch 6/99] ave_loss: 34.490096, time_elapsed: 67.928289(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: 2.457605(sec)
mae: 31.924349, best_mae: 30.248156
[epoch 7/99] ave_loss: 35.092434, time_elapsed: 73.628825(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: 2.602880(sec)
The best model 'best.pth' has been updated
mae: 30.076134, best_mae: 30.076134
[epoch 8/99] ave_loss: 32.415065, time_elapsed: 73.720619(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: 3.076798(sec)
The best model 'best.pth' has been updated
mae: 28.329926, best_mae: 28.329926
[epoch 9/99] ave_loss: 31.513645, time_elapsed: 74.451932(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: 2.499719(sec)
mae: 28.707392, best_mae: 28.329926
[epoch 10/99] ave_loss: 31.218130, time_elapsed: 83.149159(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: 3.931254(sec)
mae: 28.671291, best_mae: 28.329926
[epoch 11/99] ave_loss: 30.930822, time_elapsed: 111.991549(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: 4.844355(sec)
mae: 32.972942, best_mae: 28.329926
[epoch 12/99] ave_loss: 30.880594, time_elapsed: 125.330038(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: 4.964060(sec)
The best model 'best.pth' has been updated
mae: 27.655281, best_mae: 27.655281
[epoch 13/99] ave_loss: 30.063419, time_elapsed: 129.792774(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: 4.918992(sec)
The best model 'best.pth' has been updated
mae: 26.648949, best_mae: 26.648949
[epoch 14/99] ave_loss: 29.595541, time_elapsed: 139.251645(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: 5.654888(sec)
mae: 27.405348, best_mae: 26.648949
[epoch 15/99] ave_loss: 28.892219, time_elapsed: 156.628794(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: 5.392844(sec)
mae: 27.133324, best_mae: 26.648949
[epoch 16/99] ave_loss: 28.884909, time_elapsed: 169.180839(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: 11.292965(sec)
mae: 27.164707, best_mae: 26.648949
[epoch 17/99] ave_loss: 28.269581, time_elapsed: 173.621149(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: 6.254019(sec)
mae: 26.762152, best_mae: 26.648949
[epoch 18/99] ave_loss: 27.823765, time_elapsed: 181.729226(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: 6.227097(sec)
The best model 'best.pth' has been updated
mae: 25.728170, best_mae: 25.728170
[epoch 19/99] ave_loss: 27.478637, time_elapsed: 184.124377(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: 6.394394(sec)
mae: 28.902750, best_mae: 25.728170
[epoch 20/99] ave_loss: 27.889649, time_elapsed: 172.634653(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: 5.905251(sec)
The best model 'best.pth' has been updated
mae: 25.039875, best_mae: 25.039875
[epoch 21/99] ave_loss: 26.327826, time_elapsed: 178.333552(sec)
Validating...
num_samples: 132, num_batches: 17
time_elapsed: 11.168229(sec)
mae: 25.310787, best_mae: 25.039875
Start time: 2025-02-17 23:02:42.406423
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 8,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 148
Start time: 2025-02-17 23:03:02.512447
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 16,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 74
[epoch 0/99] ave_loss: 95.640953, time_elapsed: 56.600827(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 2.196991(sec)
The best model 'best.pth' has been updated
mae: 44.710583, best_mae: 44.710583
[epoch 1/99] ave_loss: 54.430062, time_elapsed: 45.848619(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 2.291957(sec)
The best model 'best.pth' has been updated
mae: 37.074436, best_mae: 37.074436
[epoch 2/99] ave_loss: 45.327742, time_elapsed: 50.474834(sec)
Validating...
num_samples: 132, num_batches: 9
time_elapsed: 1.872725(sec)
The best model 'best.pth' has been updated
mae: 33.556206, best_mae: 33.556206
Start time: 2025-02-17 23:06:36.843918
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 123.540106, time_elapsed: 43.531204(sec)
Validating...
num_samples: 132, num_batches: 5
time_elapsed: 2.034045(sec)
The best model 'best.pth' has been updated
mae: 56.742783, best_mae: 56.742783
[epoch 1/99] ave_loss: 68.814113, time_elapsed: 52.736959(sec)
Validating...
num_samples: 132, num_batches: 5
time_elapsed: 2.380498(sec)
The best model 'best.pth' has been updated
mae: 45.197609, best_mae: 45.197609
[epoch 2/99] ave_loss: 56.866801, time_elapsed: 40.167510(sec)
Validating...
num_samples: 132, num_batches: 5
time_elapsed: 2.255387(sec)
The best model 'best.pth' has been updated
mae: 38.864807, best_mae: 38.864807
Start time: 2025-02-17 23:20:08.543860
---------- Args ----------
{
  "dataset": "HZMetro",
  "model": "MGT",
  "name": "E01",
  "gpu": "0",
  "batch_size": 32,
  "num_workers": 8,
  "lr": 0.001,
  "eps": 1e-08,
  "weight_decay": 0.0002,
  "milestones": [
    50,
    80
  ],
  "gamma": 0.1,
  "epochs": 100,
  "val_freq": 1,
  "clip_grad_norm": false,
  "max_grad_norm": 5,
  "test": false,
  "save_every": 101,
  "dataset_model_args": {
    "dataset": {
      "root": "data/HZMetro",
      "eigenmaps_k": 8,
      "similarity_delta": 0.1
    },
    "model": {
      "d_model": 16,
      "d_k": 4,
      "d_hidden_mt": 16,
      "d_hidden_ff": 16,
      "eigenmaps_k": 8,
      "num_embeddings": [
        73,
        2
      ],
      "num_features": 2,
      "num_encoder_layers": 6,
      "num_decoder_layers": 6,
      "num_heads": 4,
      "which_transition_matrices": [
        true,
        true,
        true
      ],
      "in_len": 4,
      "out_len": 4,
      "use_curriculum_learning": false,
      "cl_decay_steps": 200,
      "dropout": 0.3,
      "noTSA": false,
      "noSSA": false,
      "noMeta": false,
      "noTE": false,
      "noSE": false
    }
  },
  "exp_dir": "exps/HZMetro/MGT/E01"
}
--------- Model Info ---------
Model size: 2.803784MB
---------- Training ----------
num_samples: 1188, num_batches: 37
[epoch 0/99] ave_loss: 126.651249, time_elapsed: 42.988797(sec)
Validating...
num_samples: 132, num_batches: 5
time_elapsed: 1.965825(sec)
The best model 'best.pth' has been updated
mae: 58.490273, best_mae: 58.490273
[epoch 1/99] ave_loss: 72.792506, time_elapsed: 46.138074(sec)
Validating...
num_samples: 132, num_batches: 5
time_elapsed: 2.154192(sec)
The best model 'best.pth' has been updated
mae: 47.090340, best_mae: 47.090340
[epoch 2/99] ave_loss: 60.769494, time_elapsed: 47.048934(sec)
Validating...
num_samples: 132, num_batches: 5
time_elapsed: 2.165622(sec)
The best model 'best.pth' has been updated
mae: 39.856609, best_mae: 39.856609
[epoch 3/99] ave_loss: 51.853179, time_elapsed: 47.404146(sec)
Validating...
num_samples: 132, num_batches: 5
time_elapsed: 2.146606(sec)
The best model 'best.pth' has been updated
mae: 36.027012, best_mae: 36.027012
[epoch 4/99] ave_loss: 46.636445, time_elapsed: 46.395997(sec)
Validating...
num_samples: 132, num_batches: 5
time_elapsed: 1.781210(sec)
The best model 'best.pth' has been updated
mae: 33.824631, best_mae: 33.824631
